{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zHgUjvEvdHyJ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 810,
     "status": "error",
     "timestamp": 1557037592052,
     "user": {
      "displayName": "Zhengyu Chen",
      "photoUrl": "https://lh6.googleusercontent.com/-fqulyiKkADI/AAAAAAAAAAI/AAAAAAAAAAc/zoRCYKQJduU/s64/photo.jpg",
      "userId": "14806602938355558554"
     },
     "user_tz": -600
    },
    "id": "L1izCgTAgmOq",
    "outputId": "9df04aae-e460-4f33-91c4-3eec2d88374a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = \"Material/train.json\"\n",
    "train_json = None\n",
    "with open(train) as t:\n",
    "    train_json = json.load(t)\n",
    "# print(train_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YX6TPLA_qvBz",
    "outputId": "7f96feaa-43ed-48b0-95a1-cda149117144"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Nikolaj', 'NNP'), ('Coster-Waldau', 'NNP'), ('worked', 'VBD'), ('with', 'IN'), ('the', 'DT'), ('Fox', 'NNP'), ('Broadcasting', 'NNP'), ('Company', 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "claim = train_json[\"75397\"][\"claim\"]\n",
    "print(nltk.pos_tag(nltk.word_tokenize(claim)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Nikolaj_Coster-Waldau', 'Fox_Broadcasting_Company']]\n",
      "[['worked_with']]\n"
     ]
    }
   ],
   "source": [
    "'''!!! this are two methods for extracting nouns and verbs. For each claim both methods will return the nouns and\n",
    "verbs respectively. By the way if the noun or verb is combined by several words, they will return them joined by \"_\"'''\n",
    "\n",
    "\n",
    "def getEntity(word_pos):\n",
    "    entity_name=[]\n",
    "    entity=[]\n",
    "    i=0\n",
    "    while(i<len(word_pos)):\n",
    "        '''extract the entity'''\n",
    "        if word_pos[i][1][:2]=='NN':\n",
    "            entity_name.append(word_pos[i][0])\n",
    "            if i == len(word_pos)-1:\n",
    "                return word_pos[i][0]\n",
    "            j=i+1\n",
    "            try:\n",
    "                while(word_pos[j][1][:2]=='NN' and j!=len(word_pos)-1):\n",
    "                    entity_name.append(word_pos[j][0])\n",
    "                    j+=1\n",
    "            except: print(word_pos,word_pos[j-1])\n",
    "            i=j\n",
    "        i+=1\n",
    "        if len(entity_name)!=0:\n",
    "            entity.append('_'.join(entity_name))\n",
    "            entity_name=[]\n",
    "    return entity\n",
    "\n",
    "def getRelation(word_pos):\n",
    "    relation_name = []\n",
    "    relation = []\n",
    "    i = 0\n",
    "    while (i < len(word_pos)):\n",
    "        '''extract the entity'''\n",
    "        if word_pos[i][1][:2] == 'VB':\n",
    "            relation_name.append(word_pos[i][0])\n",
    "            if i == len(word_pos) - 1:\n",
    "                return word_pos[i][0]\n",
    "            j = i + 1\n",
    "            while ((word_pos[j][1][:2] == 'VB' or word_pos[j][1][:2] =='RB' or word_pos[j][1][:2] =='IN')and j!=len(word_pos)-1):\n",
    "                relation_name.append(word_pos[j][0])\n",
    "                j += 1\n",
    "            i = j\n",
    "        i += 1\n",
    "        if len(relation_name) != 0:\n",
    "            relation.append('_'.join(relation_name))\n",
    "            relation_name = []\n",
    "    return relation\n",
    "\n",
    "'''for example'''\n",
    "entity=[]\n",
    "relation=[]\n",
    "entity.append(getEntity(nltk.pos_tag(nltk.word_tokenize(claim))))\n",
    "relation.append(getRelation(nltk.pos_tag(nltk.word_tokenize(claim))))\n",
    "print(entity)\n",
    "print(relation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zG2JrhbBdHyM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "# BOOM\n",
    "# folder = \"Material/wiki-pages-text\"\n",
    "folder = \".\"\n",
    "files = os.listdir(folder)\n",
    "dictionary = {}\n",
    "for file in files:\n",
    "    if not os.path.isdir(file) and file[:4] == 'wiki':\n",
    "#     if not os.path.isdir(file):\n",
    "        with open(folder + '/' + file) as f:\n",
    "            for line in f:\n",
    "                line = line.split(' ')\n",
    "                if len(line) > 2:\n",
    "                    dictionary[(line[0], line[1])] = line[2:]\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8jCJ3oNidHyO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Fact-Verification.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
