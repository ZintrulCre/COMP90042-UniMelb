{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "\n",
    "import allennlp\n",
    "# from allennlp.common.testing import AllenNlpTestCase, ModelTestCase\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.data.iterators import BucketIterator\n",
    "from allennlp.data.token_indexers import SingleIdTokenIndexer\n",
    "# from Module import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = pickle.load(open('rnn_input.txt', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'verbs': (('worked_with',), ['shortened', 'stylized_as', 'is', 'is_owned_by']), 'label': 'SUPPORTS'}, {'verbs': (('worked_with',), ['played', 'appearing_as', 'intended_as']), 'label': 'SUPPORTS'}]\n"
     ]
    }
   ],
   "source": [
    "print(raw[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for d in raw:\n",
    "    x_train.append(d['verbs'])\n",
    "    y_train.append(d['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'REFUTES', 'NOT ENOUGH INFO', 'SUPPORTS', 'SUPPORTS', 'NOT ENOUGH INFO', 'NOT ENOUGH INFO', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'REFUTES', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'REFUTES', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'NOT ENOUGH INFO', 'SUPPORTS', 'NOT ENOUGH INFO', 'NOT ENOUGH INFO', 'SUPPORTS', 'REFUTES', 'REFUTES', 'REFUTES', 'REFUTES', 'SUPPORTS', 'NOT ENOUGH INFO', 'SUPPORTS', 'NOT ENOUGH INFO', 'NOT ENOUGH INFO', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'NOT ENOUGH INFO', 'NOT ENOUGH INFO', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'REFUTES', 'SUPPORTS', 'SUPPORTS', 'REFUTES', 'SUPPORTS', 'NOT ENOUGH INFO', 'SUPPORTS', 'REFUTES', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'REFUTES', 'REFUTES', 'REFUTES', 'SUPPORTS', 'SUPPORTS', 'NOT ENOUGH INFO', 'NOT ENOUGH INFO', 'SUPPORTS', 'NOT ENOUGH INFO', 'NOT ENOUGH INFO', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'REFUTES', 'REFUTES', 'NOT ENOUGH INFO', 'SUPPORTS', 'SUPPORTS']\n"
     ]
    }
   ],
   "source": [
    "print(y_train[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SUPPORTS' 'SUPPORTS' 'SUPPORTS' 'SUPPORTS' 'SUPPORTS' 'REFUTES'\n",
      " 'NOT ENOUGH INFO' 'SUPPORTS' 'SUPPORTS' 'NOT ENOUGH INFO'\n",
      " 'NOT ENOUGH INFO' 'SUPPORTS' 'SUPPORTS' 'SUPPORTS' 'SUPPORTS' 'SUPPORTS'\n",
      " 'SUPPORTS' 'SUPPORTS' 'SUPPORTS' 'SUPPORTS']\n"
     ]
    }
   ],
   "source": [
    "y_train = np.array(y_train)\n",
    "print(y_train[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 2 2 1 0 2 2 0 0 2 2 2 2 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "# integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(y_train)\n",
    "print(integer_encoded[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:414: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "print(onehot_encoded[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('worked_with',), ['shortened', 'stylized_as', 'is', 'is_owned_by'])\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(gamma='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DatasetReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "145449it [02:09, 1125.97it/s]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from nltk.tokenize import word_tokenize\n",
    "from allennlp.data.tokenizers import Token\n",
    "from allennlp.data.fields import TextField, LabelField\n",
    "from typing import Iterator, List, Dict\n",
    "from allennlp.data import Instance\n",
    "from allennlp.data.dataset_readers import DatasetReader\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data import Vocabulary\n",
    "from allennlp.data.dataset import Batch\n",
    "\n",
    "\n",
    "class VerbDatasetReader(DatasetReader):\n",
    "\n",
    "    def __init__(self,sentence_indexers:Dict[str,TokenIndexer]=None )-> None:\n",
    "        super().__init__(lazy=False)\n",
    "        self.sentence_indexers=sentence_indexers or {\"sentence\":SingleIdTokenIndexer()}\n",
    "\n",
    "    def text_to_instance(self, claim: List[List], evidence: List[List], labels:str = None) -> Instance:\n",
    "        evidence_tokenized, claim_tokenized = [], []\n",
    "        for c in claim:\n",
    "            for word in word_tokenize(c):\n",
    "                claim_tokenized.append(Token(word))\n",
    "        for e in evidence:\n",
    "            for word in word_tokenize(e):\n",
    "                evidence_tokenized.append(Token(word))\n",
    "        claim_textfiled = TextField(claim_tokenized, self.sentence_indexers)\n",
    "        evidence_textfiled = TextField(evidence_tokenized, self.sentence_indexers)\n",
    "        fields={'claim':claim_textfiled, 'evidence':evidence_textfiled,'label': LabelField(labels)}\n",
    "        return Instance(fields)\n",
    "\n",
    "    def _read(self, file_path: str)->Iterator[Instance]:\n",
    "        mlinput_merge=pickle.load(open(file_path,'rb'))\n",
    "        for entry in mlinput_merge:\n",
    "            claim = [entry['claim']]\n",
    "            evidence = []\n",
    "            for sentence in entry['evidence']:\n",
    "                sentence = ' '.join(sentence)\n",
    "                evidence.append(sentence)\n",
    "            yield self.text_to_instance(claim, evidence, entry['label'])\n",
    "\n",
    "\n",
    "\n",
    "reader=VerbDatasetReader()\n",
    "\n",
    "instances = reader.read('mlinput_merge.txt')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': [The,\n",
       "  Fox,\n",
       "  Broadcasting,\n",
       "  Company,\n",
       "  -LRB-,\n",
       "  often,\n",
       "  shortened,\n",
       "  to,\n",
       "  Fox,\n",
       "  and,\n",
       "  stylized,\n",
       "  as,\n",
       "  FOX,\n",
       "  -RRB-,\n",
       "  is,\n",
       "  an,\n",
       "  American,\n",
       "  English,\n",
       "  language,\n",
       "  commercial,\n",
       "  broadcast,\n",
       "  television,\n",
       "  network,\n",
       "  that,\n",
       "  is,\n",
       "  owned,\n",
       "  by,\n",
       "  the,\n",
       "  Fox,\n",
       "  Entertainment,\n",
       "  Group,\n",
       "  subsidiary,\n",
       "  of,\n",
       "  21st,\n",
       "  Century,\n",
       "  Fox,\n",
       "  .,\n",
       "  He,\n",
       "  then,\n",
       "  played,\n",
       "  Detective,\n",
       "  John,\n",
       "  Amsterdam,\n",
       "  in,\n",
       "  the,\n",
       "  short-lived,\n",
       "  Fox,\n",
       "  television,\n",
       "  series,\n",
       "  New,\n",
       "  Amsterdam,\n",
       "  -LRB-,\n",
       "  2008,\n",
       "  -RRB-,\n",
       "  ,,\n",
       "  as,\n",
       "  well,\n",
       "  as,\n",
       "  appearing,\n",
       "  as,\n",
       "  Frank,\n",
       "  Pike,\n",
       "  in,\n",
       "  the,\n",
       "  2009,\n",
       "  Fox,\n",
       "  television,\n",
       "  film,\n",
       "  Virtuality,\n",
       "  ,,\n",
       "  originally,\n",
       "  intended,\n",
       "  as,\n",
       "  a,\n",
       "  pilot,\n",
       "  .],\n",
       " '_token_indexers': {'sentence': <allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer at 0x17ac22c50>},\n",
       " '_indexed_tokens': None,\n",
       " '_indexer_name_to_indexed_token': None}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(instances[0].fields['evidence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 145449/145449 [00:06<00:00, 22042.80it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocabulary.from_instances(instances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding\n",
    "token_embedding = Embedding(num_embeddings=config.max_vocab_size + 2,\n",
    "                            embedding_dim=100, padding_index=0)\n",
    "# the embedder maps the input tokens to the appropriate embedding matrix\n",
    "word_embeddings: TextFieldEmbedder = BasicTextFieldEmbedder({\"tokens\": token_embedding})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute '_six'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-118-81e78ac8cbdd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_field_embedders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBasicTextFieldEmbedder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mclaim_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# evidence_embedding = Embedding(num_embeddings=vocab.get_vocab_size(\"evidence\"), embedding_dim=100, padding_index=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mevidence_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/allennlp/modules/token_embedders/embedding.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_embeddings, embedding_dim, projection_dim, weight, padding_index, trainable, max_norm, norm_type, scale_grad_by_freq, sparse, vocab_namespace, pretrained_file)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxavier_uniform_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m    551\u001b[0m                     \"cannot assign parameters before Module.__init__() call\")\n\u001b[1;32m    552\u001b[0m             \u001b[0mremove_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_parameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mregister_parameter\u001b[0;34m(self, name, param)\u001b[0m\n\u001b[1;32m    140\u001b[0m                 \"cannot assign parameter before Module.__init__() call\")\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m             raise TypeError(\"parameter name should be a string. \"\n\u001b[1;32m    144\u001b[0m                             \"Got {}\".format(torch.typename(name)))\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute '_six'"
     ]
    }
   ],
   "source": [
    "# Basic embeddings\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
    "\n",
    "claim_embedding = Embedding(num_embeddings=test_size, embedding_dim=100)\n",
    "# evidence_embedding = Embedding(num_embeddings=vocab.get_vocab_size(\"evidence\"), embedding_dim=100, padding_index=0)\n",
    "evidence_embedding = Embedding(num_embeddings=test_size, embedding_dim=100, padding_index=0)\n",
    "\n",
    "# the embedder maps the input tokens to the appropriate embedding matrix\n",
    "word_embeddings: TextFieldEmbedder = BasicTextFieldEmbedder({\"tokens\": token_embedding})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = BucketIterator(batch_size=config.batch_size, \n",
    "                          sorting_keys=[(\"claim\", \"num_tokens\"), (\"evidence\", \"num_tokens\")],\n",
    "                         )\n",
    "iterator.index_with(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(iterator(training_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-108-a80532d9bc11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mAcademicPaperClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     def __init__(self,\n\u001b[1;32m      3\u001b[0m                  \u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mVocabulary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                  \u001b[0mtext_field_embedder\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTextFieldEmbedder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                  \u001b[0mtitle_encoder\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSeq2VecEncoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Model' is not defined"
     ]
    }
   ],
   "source": [
    "class AcademicPaperClassifier(Model):\n",
    "    def __init__(self,\n",
    "                 vocab: Vocabulary,\n",
    "                 text_field_embedder: TextFieldEmbedder,\n",
    "                 title_encoder: Seq2VecEncoder,\n",
    "                 abstract_encoder: Seq2VecEncoder,\n",
    "                 classifier_feedforward: FeedForward,\n",
    "                 initializer: InitializerApplicator = InitializerApplicator(),\n",
    "                 regularizer: Optional[RegularizerApplicator] = None) -> None:\n",
    "        super(AcademicPaperClassifier, self).__init__(vocab, regularizer)\n",
    "\n",
    "        self.text_field_embedder = text_field_embedder\n",
    "        self.num_classes = self.vocab.get_vocab_size(\"labels\")\n",
    "        self.title_encoder = title_encoder\n",
    "        self.abstract_encoder = abstract_encoder\n",
    "        self.classifier_feedforward = classifier_feedforward\n",
    "        self.metrics = {\n",
    "                \"accuracy\": CategoricalAccuracy(),\n",
    "                \"accuracy3\": CategoricalAccuracy(top_k=3)\n",
    "        }\n",
    "        self.loss = torch.nn.CrossEntropyLoss()\n",
    "        initializer(self)\n",
    "        \n",
    "    def forward(self, title: Dict[str, torch.LongTensor], abstract: Dict[str, torch.LongTensor],\n",
    "                label: torch.LongTensor = None) -> Dict[str, torch.Tensor]:\n",
    "        embedded_title = self.text_field_embedder(title)\n",
    "        title_mask = util.get_text_field_mask(title)\n",
    "        encoded_title = self.title_encoder(embedded_title, title_mask)\n",
    "\n",
    "        embedded_abstract = self.text_field_embedder(abstract)\n",
    "        abstract_mask = util.get_text_field_mask(abstract)\n",
    "        encoded_abstract = self.abstract_encoder(embedded_abstract, abstract_mask)\n",
    "\n",
    "        logits = self.classifier_feedforward(torch.cat([encoded_title, encoded_abstract], dim=-1))\n",
    "        class_probabilities = F.softmax(logits)\n",
    "\n",
    "        output_dict = {\"class_probabilities\": class_probabilities}\n",
    "\n",
    "        if label is not None:\n",
    "            loss = self.loss(logits, label.squeeze(-1))\n",
    "            for metric in self.metrics.values():\n",
    "                metric(logits, label.squeeze(-1))\n",
    "            output_dict[\"loss\"] = loss\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        return {metric_name: metric.get_metric(reset) for metric_name, metric in self.metrics.items()}\n",
    "This method is how the model tells the training code which metrics it's computing. The second pieces of code is the decode method:\n",
    "\n",
    "    def decode(self, output_dict: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        predictions = output_dict['class_probabilities'].cpu().data.numpy()\n",
    "        argmax_indices = numpy.argmax(predictions, axis=-1)\n",
    "        labels = [self.vocab.get_token_from_index(x, namespace=\"labels\")\n",
    "                  for x in argmax_indices]\n",
    "        output_dict['label'] = labels\n",
    "        return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance with fields:\n",
      " \t evidece: TextField of length 85 with text: \n",
      " \t\t[Nikolaj, Coster-Waldau, worked, with, the, Fox, Broadcasting, Company, ., The, Fox, Broadcasting,\n",
      "\t\tCompany, -LRB-, often, shortened, to, Fox, and, stylized, as, FOX, -RRB-, is, an, American, English,\n",
      "\t\tlanguage, commercial, broadcast, television, network, that, is, owned, by, the, Fox, Entertainment,\n",
      "\t\tGroup, subsidiary, of, 21st, Century, Fox, ., He, then, played, Detective, John, Amsterdam, in, the,\n",
      "\t\tshort-lived, Fox, television, series, New, Amsterdam, -LRB-, 2008, -RRB-, ,, as, well, as,\n",
      "\t\tappearing, as, Frank, Pike, in, the, 2009, Fox, television, film, Virtuality, ,, originally,\n",
      "\t\tintended, as, a, pilot, .]\n",
      " \t\tand TokenIndexers : {'sentence': 'SingleIdTokenIndexer'} \n",
      " \t label: LabelField with label: SUPPORTS in namespace: 'labels'.' \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'claim': 'Nikolaj Coster-Waldau worked with the Fox Broadcasting Company.', 'label': 'SUPPORTS', 'evidence': [['The', 'Fox', 'Broadcasting', 'Company', '-LRB-', 'often', 'shortened', 'to', 'Fox', 'and', 'stylized', 'as', 'FOX', '-RRB-', 'is', 'an', 'American', 'English', 'language', 'commercial', 'broadcast', 'television', 'network', 'that', 'is', 'owned', 'by', 'the', 'Fox', 'Entertainment', 'Group', 'subsidiary', 'of', '21st', 'Century', 'Fox', '.\\n'], ['He', 'then', 'played', 'Detective', 'John', 'Amsterdam', 'in', 'the', 'short-lived', 'Fox', 'television', 'series', 'New', 'Amsterdam', '-LRB-', '2008', '-RRB-', ',', 'as', 'well', 'as', 'appearing', 'as', 'Frank', 'Pike', 'in', 'the', '2009', 'Fox', 'television', 'film', 'Virtuality', ',', 'originally', 'intended', 'as', 'a', 'pilot', '.\\n']]}\n"
     ]
    }
   ],
   "source": [
    "with open('mlinput_merge.txt', 'rb') as m:\n",
    "    a = pickle.load(m)\n",
    "    print(a[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestSemanticScholarDatasetReader(AllenNlpTestCase):\n",
    "    def test_read_from_file(self):\n",
    "        reader = SemanticScholarDatasetReader()\n",
    "        dataset = reader.read('tests/fixtures/s2_papers.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
