{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "\n",
    "import allennlp\n",
    "# from allennlp.common.testing import AllenNlpTestCase, ModelTestCase\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.data.iterators import BucketIterator\n",
    "from allennlp.data.token_indexers import SingleIdTokenIndexer\n",
    "# from Module import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = pickle.load(open('rnn_input.txt', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'verbs': (('worked_with',), ['shortened', 'stylized_as', 'is', 'is_owned_by']), 'label': 'SUPPORTS'}, {'verbs': (('worked_with',), ['played', 'appearing_as', 'intended_as']), 'label': 'SUPPORTS'}]\n"
     ]
    }
   ],
   "source": [
    "print(raw[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for d in raw:\n",
    "    x_train.append(d['verbs'])\n",
    "    y_train.append(d['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'REFUTES', 'NOT ENOUGH INFO', 'SUPPORTS', 'SUPPORTS', 'NOT ENOUGH INFO', 'NOT ENOUGH INFO', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'REFUTES', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'REFUTES', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'NOT ENOUGH INFO', 'SUPPORTS', 'NOT ENOUGH INFO', 'NOT ENOUGH INFO', 'SUPPORTS', 'REFUTES', 'REFUTES', 'REFUTES', 'REFUTES', 'SUPPORTS', 'NOT ENOUGH INFO', 'SUPPORTS', 'NOT ENOUGH INFO', 'NOT ENOUGH INFO', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'NOT ENOUGH INFO', 'NOT ENOUGH INFO', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'REFUTES', 'SUPPORTS', 'SUPPORTS', 'REFUTES', 'SUPPORTS', 'NOT ENOUGH INFO', 'SUPPORTS', 'REFUTES', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'REFUTES', 'REFUTES', 'REFUTES', 'SUPPORTS', 'SUPPORTS', 'NOT ENOUGH INFO', 'NOT ENOUGH INFO', 'SUPPORTS', 'NOT ENOUGH INFO', 'NOT ENOUGH INFO', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'SUPPORTS', 'REFUTES', 'REFUTES', 'NOT ENOUGH INFO', 'SUPPORTS', 'SUPPORTS']\n"
     ]
    }
   ],
   "source": [
    "print(y_train[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SUPPORTS' 'SUPPORTS' 'SUPPORTS' 'SUPPORTS' 'SUPPORTS' 'REFUTES'\n",
      " 'NOT ENOUGH INFO' 'SUPPORTS' 'SUPPORTS' 'NOT ENOUGH INFO'\n",
      " 'NOT ENOUGH INFO' 'SUPPORTS' 'SUPPORTS' 'SUPPORTS' 'SUPPORTS' 'SUPPORTS'\n",
      " 'SUPPORTS' 'SUPPORTS' 'SUPPORTS' 'SUPPORTS']\n"
     ]
    }
   ],
   "source": [
    "y_train = np.array(y_train)\n",
    "print(y_train[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 2 2 1 0 2 2 0 0 2 2 2 2 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "# integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(y_train)\n",
    "print(integer_encoded[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:414: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "print(onehot_encoded[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('worked_with',), ['shortened', 'stylized_as', 'is', 'is_owned_by'])\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(gamma='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:00,  2.40it/s]\u001b[A\n",
      "105it [00:00,  3.42it/s]\u001b[A\n",
      "237it [00:00,  4.89it/s]\u001b[A\n",
      "350it [00:00,  6.97it/s]\u001b[A\n",
      "474it [00:00,  9.93it/s]\u001b[A\n",
      "608it [00:00, 14.14it/s]\u001b[A\n",
      "740it [00:01, 20.10it/s]\u001b[A\n",
      "867it [00:01, 28.53it/s]\u001b[A\n",
      "995it [00:01, 40.37it/s]\u001b[A\n",
      "1124it [00:01, 56.90it/s]\u001b[A\n",
      "1247it [00:01, 79.64it/s]\u001b[A\n",
      "1373it [00:01, 110.77it/s]\u001b[A\n",
      "1502it [00:01, 152.63it/s]\u001b[A\n",
      "1633it [00:01, 207.47it/s]\u001b[A\n",
      "1759it [00:01, 276.45it/s]\u001b[A\n",
      "1885it [00:01, 360.87it/s]\u001b[A\n",
      "2013it [00:02, 459.74it/s]\u001b[A\n",
      "2142it [00:02, 569.43it/s]\u001b[A\n",
      "2282it [00:02, 689.55it/s]\u001b[A\n",
      "2412it [00:02, 797.51it/s]\u001b[A\n",
      "2541it [00:02, 890.24it/s]\u001b[A\n",
      "2680it [00:02, 996.69it/s]\u001b[A\n",
      "2810it [00:02, 1037.01it/s]\u001b[A\n",
      "2938it [00:02, 1095.78it/s]\u001b[A\n",
      "3080it [00:02, 1175.32it/s]\u001b[A\n",
      "3224it [00:02, 1243.39it/s]\u001b[A\n",
      "3362it [00:03, 1279.28it/s]\u001b[A\n",
      "3498it [00:03, 1294.49it/s]\u001b[A\n",
      "3633it [00:03, 1295.00it/s]\u001b[A\n",
      "3767it [00:03, 1248.62it/s]\u001b[A\n",
      "3895it [00:03, 1253.80it/s]\u001b[A\n",
      "4023it [00:03, 1209.70it/s]\u001b[A\n",
      "4151it [00:03, 1229.31it/s]\u001b[A\n",
      "4276it [00:03, 1184.69it/s]\u001b[A\n",
      "4407it [00:03, 1216.80it/s]\u001b[A\n",
      "4538it [00:04, 1242.30it/s]\u001b[A\n",
      "4674it [00:04, 1272.88it/s]\u001b[A\n",
      "4810it [00:04, 1283.02it/s]\u001b[A\n",
      "4939it [00:04, 1266.61it/s]\u001b[A\n",
      "5070it [00:04, 1277.94it/s]\u001b[A\n",
      "5199it [00:04, 1197.29it/s]\u001b[A\n",
      "5321it [00:04, 1157.37it/s]\u001b[A\n",
      "5452it [00:04, 1197.44it/s]\u001b[A\n",
      "5596it [00:04, 1260.52it/s]\u001b[A\n",
      "5730it [00:04, 1274.02it/s]\u001b[A\n",
      "5859it [00:05, 1275.39it/s]\u001b[A\n",
      "6004it [00:05, 1322.16it/s]\u001b[A\n",
      "6149it [00:05, 1353.63it/s]\u001b[A\n",
      "6286it [00:05, 1327.49it/s]\u001b[A\n",
      "6440it [00:05, 1384.61it/s]\u001b[A\n",
      "6580it [00:05, 1370.22it/s]\u001b[A\n",
      "6718it [00:05, 1344.22it/s]\u001b[A\n",
      "6854it [00:05, 1303.70it/s]\u001b[A\n",
      "6997it [00:05, 1337.71it/s]\u001b[A\n",
      "7132it [00:06, 1320.53it/s]\u001b[A\n",
      "7265it [00:06, 1308.56it/s]\u001b[A\n",
      "7400it [00:06, 1318.88it/s]\u001b[A\n",
      "7533it [00:06, 1299.68it/s]\u001b[A\n",
      "7673it [00:06, 1324.72it/s]\u001b[A\n",
      "7806it [00:06, 1310.30it/s]\u001b[A\n",
      "7938it [00:06, 1311.44it/s]\u001b[A\n",
      "8085it [00:06, 1353.09it/s]\u001b[A\n",
      "8221it [00:06, 1247.35it/s]\u001b[A\n",
      "8363it [00:06, 1293.14it/s]\u001b[A\n",
      "8495it [00:07, 1259.13it/s]\u001b[A\n",
      "8623it [00:07, 1221.16it/s]\u001b[A\n",
      "8769it [00:07, 1281.68it/s]\u001b[A\n",
      "8899it [00:07, 1272.79it/s]\u001b[A\n",
      "9028it [00:07, 1259.60it/s]\u001b[A\n",
      "9155it [00:07, 1232.56it/s]\u001b[A\n",
      "9280it [00:07, 1234.56it/s]\u001b[A\n",
      "9404it [00:07, 1219.22it/s]\u001b[A\n",
      "9539it [00:07, 1251.41it/s]\u001b[A\n",
      "9680it [00:07, 1294.34it/s]\u001b[A\n",
      "9821it [00:08, 1325.39it/s]\u001b[A\n",
      "9955it [00:08, 1304.05it/s]\u001b[A\n",
      "10086it [00:08, 1295.05it/s]\u001b[A\n",
      "10216it [00:08, 1292.65it/s]\u001b[A\n",
      "10346it [00:08, 1275.72it/s]\u001b[A\n",
      "10476it [00:08, 1279.83it/s]\u001b[A\n",
      "10614it [00:08, 1306.58it/s]\u001b[A\n",
      "10745it [00:08, 1283.05it/s]\u001b[A\n",
      "10883it [00:08, 1308.22it/s]\u001b[A\n",
      "11034it [00:09, 1361.13it/s]\u001b[A\n",
      "11171it [00:09, 1357.30it/s]\u001b[A\n",
      "11315it [00:09, 1380.34it/s]\u001b[A\n",
      "11454it [00:09, 1356.43it/s]\u001b[A\n",
      "11606it [00:09, 1400.81it/s]\u001b[A\n",
      "11747it [00:09, 1375.90it/s]\u001b[A\n",
      "11895it [00:09, 1400.64it/s]\u001b[A\n",
      "12042it [00:09, 1419.16it/s]\u001b[A\n",
      "12185it [00:09, 1365.75it/s]\u001b[A\n",
      "12338it [00:09, 1410.39it/s]\u001b[A\n",
      "12488it [00:10, 1434.28it/s]\u001b[A\n",
      "12633it [00:10, 1329.52it/s]\u001b[A\n",
      "12768it [00:10, 1324.62it/s]\u001b[A\n",
      "12908it [00:10, 1342.31it/s]\u001b[A\n",
      "13044it [00:10, 1339.11it/s]\u001b[A\n",
      "13196it [00:10, 1387.16it/s]\u001b[A\n",
      "13336it [00:10, 1377.94it/s]\u001b[A\n",
      "13475it [00:10, 1362.51it/s]\u001b[A\n",
      "13624it [00:10, 1398.28it/s]\u001b[A\n",
      "13775it [00:10, 1429.54it/s]\u001b[A\n",
      "13919it [00:11, 1359.80it/s]\u001b[A\n",
      "14057it [00:11, 1364.12it/s]\u001b[A\n",
      "14197it [00:11, 1373.28it/s]\u001b[A\n",
      "14335it [00:11, 1355.05it/s]\u001b[A\n",
      "14483it [00:11, 1390.13it/s]\u001b[A\n",
      "14630it [00:11, 1409.66it/s]\u001b[A\n",
      "14780it [00:11, 1432.15it/s]\u001b[A\n",
      "14924it [00:11, 1400.22it/s]\u001b[A\n",
      "15066it [00:11, 1405.10it/s]\u001b[A\n",
      "15234it [00:12, 1475.19it/s]\u001b[A\n",
      "15383it [00:12, 1393.33it/s]\u001b[A\n",
      "15525it [00:12, 1375.28it/s]\u001b[A\n",
      "15668it [00:12, 1390.61it/s]\u001b[A\n",
      "15822it [00:12, 1427.76it/s]\u001b[A\n",
      "15966it [00:12, 1345.61it/s]\u001b[A\n",
      "16103it [00:12, 1350.01it/s]\u001b[A\n",
      "16245it [00:12, 1368.65it/s]\u001b[A\n",
      "16387it [00:12, 1376.58it/s]\u001b[A\n",
      "16528it [00:12, 1384.58it/s]\u001b[A\n",
      "16671it [00:13, 1393.16it/s]\u001b[A\n",
      "16815it [00:13, 1405.31it/s]\u001b[A\n",
      "16961it [00:13, 1421.04it/s]\u001b[A\n",
      "17107it [00:13, 1432.51it/s]\u001b[A\n",
      "17256it [00:13, 1448.93it/s]\u001b[A\n",
      "17403it [00:13, 1454.85it/s]\u001b[A\n",
      "17550it [00:13, 1457.96it/s]\u001b[A\n",
      "17708it [00:13, 1490.01it/s]\u001b[A\n",
      "17858it [00:13, 1489.87it/s]\u001b[A\n",
      "18008it [00:13, 1470.31it/s]\u001b[A\n",
      "18156it [00:14, 1277.00it/s]\u001b[A\n",
      "18289it [00:14, 1289.17it/s]\u001b[A\n",
      "18422it [00:14, 1234.59it/s]\u001b[A\n",
      "18569it [00:14, 1296.28it/s]\u001b[A\n",
      "18702it [00:14, 1305.80it/s]\u001b[A\n",
      "18835it [00:14, 1224.26it/s]\u001b[A\n",
      "18960it [00:14, 1215.30it/s]\u001b[A\n",
      "19084it [00:14, 1218.69it/s]\u001b[A\n",
      "19209it [00:14, 1227.46it/s]\u001b[A\n",
      "19333it [00:15, 1226.26it/s]\u001b[A\n",
      "19457it [00:15, 1213.75it/s]\u001b[A\n",
      "19581it [00:15, 1220.10it/s]\u001b[A\n",
      "19704it [00:15, 1207.29it/s]\u001b[A\n",
      "19830it [00:15, 1219.58it/s]\u001b[A\n",
      "19953it [00:15, 1188.21it/s]\u001b[A\n",
      "20087it [00:15, 1229.90it/s]\u001b[A\n",
      "20213it [00:15, 1234.62it/s]\u001b[A\n",
      "20366it [00:15, 1309.73it/s]\u001b[A\n",
      "20509it [00:16, 1342.71it/s]\u001b[A\n",
      "20645it [00:16, 1259.73it/s]\u001b[A\n",
      "20773it [00:16, 1262.44it/s]\u001b[A\n",
      "20901it [00:16, 1244.68it/s]\u001b[A\n",
      "21030it [00:16, 1257.61it/s]\u001b[A\n",
      "21165it [00:16, 1281.97it/s]\u001b[A\n",
      "21294it [00:16, 1222.46it/s]\u001b[A\n",
      "21441it [00:16, 1286.86it/s]\u001b[A\n",
      "21583it [00:16, 1322.70it/s]\u001b[A\n",
      "21719it [00:16, 1333.58it/s]\u001b[A\n",
      "21854it [00:17, 1324.18it/s]\u001b[A\n",
      "21990it [00:17, 1332.88it/s]\u001b[A\n",
      "22124it [00:17, 1328.43it/s]\u001b[A\n",
      "22258it [00:17, 1327.64it/s]\u001b[A\n",
      "22407it [00:17, 1370.94it/s]\u001b[A\n",
      "22545it [00:17, 1368.64it/s]\u001b[A\n",
      "22683it [00:17, 1352.31it/s]\u001b[A\n",
      "22828it [00:17, 1376.50it/s]\u001b[A\n",
      "22966it [00:17, 1337.90it/s]\u001b[A\n",
      "23102it [00:17, 1342.70it/s]\u001b[A\n",
      "23237it [00:18, 1321.67it/s]\u001b[A\n",
      "23370it [00:18, 1314.21it/s]\u001b[A\n",
      "23502it [00:18, 1290.46it/s]\u001b[A\n",
      "23646it [00:18, 1330.73it/s]\u001b[A\n",
      "23784it [00:18, 1344.09it/s]\u001b[A\n",
      "23919it [00:18, 1331.90it/s]\u001b[A\n",
      "24060it [00:18, 1351.30it/s]\u001b[A\n",
      "24196it [00:18, 1326.45it/s]\u001b[A\n",
      "24329it [00:18, 1317.08it/s]\u001b[A\n",
      "24488it [00:19, 1378.81it/s]\u001b[A\n",
      "24630it [00:19, 1390.88it/s]\u001b[A\n",
      "24770it [00:19, 1390.04it/s]\u001b[A\n",
      "24919it [00:19, 1418.28it/s]\u001b[A\n",
      "25065it [00:19, 1429.26it/s]\u001b[A\n",
      "25214it [00:19, 1442.49it/s]\u001b[A\n",
      "25359it [00:19, 1414.36it/s]\u001b[A\n",
      "25501it [00:19, 1389.26it/s]\u001b[A\n",
      "25647it [00:19, 1408.65it/s]\u001b[A\n",
      "25806it [00:19, 1456.96it/s]\u001b[A\n",
      "25953it [00:20, 1425.14it/s]\u001b[A\n",
      "26097it [00:20, 1372.58it/s]\u001b[A\n",
      "26240it [00:20, 1386.88it/s]\u001b[A\n",
      "26387it [00:20, 1408.34it/s]\u001b[A\n",
      "26529it [00:20, 1388.13it/s]\u001b[A\n",
      "26669it [00:20, 1366.59it/s]\u001b[A\n",
      "26809it [00:20, 1374.57it/s]\u001b[A\n",
      "26947it [00:20, 1374.80it/s]\u001b[A\n",
      "27085it [00:20, 1267.66it/s]\u001b[A\n",
      "27214it [00:21, 1243.03it/s]\u001b[A\n",
      "27341it [00:21, 1245.63it/s]\u001b[A\n",
      "27474it [00:21, 1267.37it/s]\u001b[A\n",
      "27603it [00:21, 1273.15it/s]\u001b[A\n",
      "27731it [00:21, 1242.38it/s]\u001b[A\n",
      "27873it [00:21, 1290.00it/s]\u001b[A\n",
      "28003it [00:21, 1257.59it/s]\u001b[A\n",
      "28144it [00:21, 1297.67it/s]\u001b[A\n",
      "28280it [00:21, 1315.56it/s]\u001b[A\n",
      "28417it [00:21, 1321.80it/s]\u001b[A\n",
      "28563it [00:22, 1359.96it/s]\u001b[A\n",
      "28700it [00:22, 1275.05it/s]\u001b[A\n",
      "28838it [00:22, 1304.52it/s]\u001b[A\n",
      "28970it [00:22, 1220.20it/s]\u001b[A\n",
      "29097it [00:22, 1233.32it/s]\u001b[A\n",
      "29223it [00:22, 1239.21it/s]\u001b[A\n",
      "29348it [00:22, 1177.87it/s]\u001b[A\n",
      "29487it [00:22, 1232.34it/s]\u001b[A\n",
      "29617it [00:22, 1250.15it/s]\u001b[A\n",
      "29748it [00:23, 1266.92it/s]\u001b[A\n",
      "29878it [00:23, 1275.81it/s]\u001b[A\n",
      "30007it [00:23, 1250.49it/s]\u001b[A\n",
      "30143it [00:23, 1279.05it/s]\u001b[A\n",
      "30284it [00:23, 1314.47it/s]\u001b[A\n",
      "30417it [00:23, 1305.23it/s]\u001b[A\n",
      "30570it [00:23, 1362.45it/s]\u001b[A\n",
      "30708it [00:23, 1323.16it/s]\u001b[A\n",
      "30842it [00:23, 1256.51it/s]\u001b[A\n",
      "30973it [00:23, 1266.08it/s]\u001b[A\n",
      "31123it [00:24, 1327.84it/s]\u001b[A\n",
      "31258it [00:24, 1298.53it/s]\u001b[A\n",
      "31393it [00:24, 1312.34it/s]\u001b[A\n",
      "31526it [00:24, 1278.61it/s]\u001b[A\n",
      "31655it [00:24, 1262.18it/s]\u001b[A\n",
      "31804it [00:24, 1322.17it/s]\u001b[A\n",
      "31938it [00:24, 1306.51it/s]\u001b[A\n",
      "32070it [00:24, 1271.24it/s]\u001b[A\n",
      "32198it [00:24, 1200.94it/s]\u001b[A\n",
      "32339it [00:25, 1254.73it/s]\u001b[A\n",
      "32467it [00:25, 1247.20it/s]\u001b[A\n",
      "32594it [00:25, 1249.32it/s]\u001b[A\n",
      "32721it [00:25, 1252.58it/s]\u001b[A\n",
      "32853it [00:25, 1270.49it/s]\u001b[A\n",
      "32990it [00:25, 1298.08it/s]\u001b[A\n",
      "33125it [00:25, 1312.78it/s]\u001b[A\n",
      "33270it [00:25, 1349.16it/s]\u001b[A\n",
      "33414it [00:25, 1365.08it/s]\u001b[A\n",
      "33554it [00:25, 1375.01it/s]\u001b[A\n",
      "33692it [00:26, 1372.14it/s]\u001b[A\n",
      "33837it [00:26, 1391.24it/s]\u001b[A\n",
      "33981it [00:26, 1405.06it/s]\u001b[A\n",
      "34122it [00:26, 1393.91it/s]\u001b[A\n",
      "34262it [00:26, 1376.85it/s]\u001b[A\n",
      "34400it [00:26, 1317.86it/s]\u001b[A\n",
      "34548it [00:26, 1362.16it/s]\u001b[A\n",
      "34697it [00:26, 1397.11it/s]\u001b[A\n",
      "34838it [00:26, 1365.43it/s]\u001b[A\n",
      "34976it [00:26, 1356.19it/s]\u001b[A\n",
      "35122it [00:27, 1383.17it/s]\u001b[A\n",
      "35261it [00:27, 1337.64it/s]\u001b[A\n",
      "35396it [00:27, 1327.53it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "35544it [00:27, 1367.87it/s]\u001b[A\n",
      "35682it [00:27, 1358.12it/s]\u001b[A\n",
      "35819it [00:27, 1353.58it/s]\u001b[A\n",
      "35955it [00:27, 1322.58it/s]\u001b[A\n",
      "36093it [00:27, 1338.15it/s]\u001b[A\n",
      "36241it [00:27, 1377.48it/s]\u001b[A\n",
      "36380it [00:28, 1310.95it/s]\u001b[A\n",
      "36516it [00:28, 1324.18it/s]\u001b[A\n",
      "36650it [00:28, 1300.48it/s]\u001b[A\n",
      "36789it [00:28, 1323.57it/s]\u001b[A\n",
      "36923it [00:28, 1314.17it/s]\u001b[A\n",
      "37055it [00:28, 1296.97it/s]\u001b[A\n",
      "37186it [00:28, 1278.10it/s]\u001b[A\n",
      "37315it [00:28, 1265.02it/s]\u001b[A\n",
      "37455it [00:28, 1301.72it/s]\u001b[A\n",
      "37586it [00:28, 1294.38it/s]\u001b[A\n",
      "37716it [00:29, 1290.13it/s]\u001b[A\n",
      "37862it [00:29, 1335.67it/s]\u001b[A\n",
      "37997it [00:29, 1339.03it/s]\u001b[A\n",
      "38132it [00:29, 1309.72it/s]\u001b[A\n",
      "38264it [00:29, 1296.53it/s]\u001b[A\n",
      "38394it [00:29, 1290.72it/s]\u001b[A\n",
      "38525it [00:29, 1295.60it/s]\u001b[A\n",
      "38661it [00:29, 1310.86it/s]\u001b[A\n",
      "38793it [00:29, 1310.87it/s]\u001b[A\n",
      "38925it [00:29, 1305.76it/s]\u001b[A\n",
      "39056it [00:30, 1256.02it/s]\u001b[A\n",
      "39184it [00:30, 1258.11it/s]\u001b[A\n",
      "39311it [00:30, 1237.73it/s]\u001b[A\n",
      "39457it [00:30, 1296.23it/s]\u001b[A\n",
      "39602it [00:30, 1335.51it/s]\u001b[A\n",
      "39741it [00:30, 1348.83it/s]\u001b[A\n",
      "39877it [00:30, 1326.12it/s]\u001b[A\n",
      "40016it [00:30, 1343.39it/s]\u001b[A\n",
      "40151it [00:30, 1324.50it/s]\u001b[A\n",
      "40284it [00:30, 1306.72it/s]\u001b[A\n",
      "40415it [00:31, 1276.40it/s]\u001b[A\n",
      "40544it [00:31, 1274.07it/s]\u001b[A\n",
      "40680it [00:31, 1298.22it/s]\u001b[A\n",
      "40821it [00:31, 1324.81it/s]\u001b[A\n",
      "40959it [00:31, 1337.72it/s]\u001b[A\n",
      "41094it [00:31, 1323.68it/s]\u001b[A\n",
      "41240it [00:31, 1359.42it/s]\u001b[A\n",
      "41377it [00:31, 1353.48it/s]\u001b[A\n",
      "41513it [00:31, 1324.74it/s]\u001b[A\n",
      "41655it [00:32, 1328.88it/s]\u001b[A\n",
      "41791it [00:32, 1330.88it/s]\u001b[A\n",
      "41925it [00:32, 1285.29it/s]\u001b[A\n",
      "42054it [00:32, 1274.37it/s]\u001b[A\n",
      "42182it [00:32, 1243.32it/s]\u001b[A\n",
      "42314it [00:32, 1265.05it/s]\u001b[A\n",
      "42451it [00:32, 1292.61it/s]\u001b[A\n",
      "42581it [00:32, 1281.69it/s]\u001b[A\n",
      "42710it [00:32, 1276.38it/s]\u001b[A\n",
      "42838it [00:32, 1223.85it/s]\u001b[A\n",
      "42962it [00:33, 1215.19it/s]\u001b[A\n",
      "43092it [00:33, 1236.58it/s]\u001b[A\n",
      "43217it [00:33, 1235.05it/s]\u001b[A\n",
      "43341it [00:33, 1226.38it/s]\u001b[A\n",
      "43464it [00:33, 1221.18it/s]\u001b[A\n",
      "43590it [00:33, 1230.91it/s]\u001b[A\n",
      "43736it [00:33, 1291.59it/s]\u001b[A\n",
      "43870it [00:33, 1304.46it/s]\u001b[A\n",
      "44003it [00:33, 1311.25it/s]\u001b[A\n",
      "44135it [00:33, 1285.63it/s]\u001b[A\n",
      "44280it [00:34, 1330.30it/s]\u001b[A\n",
      "44414it [00:34, 1271.44it/s]\u001b[A\n",
      "44543it [00:34, 1264.34it/s]\u001b[A\n",
      "44678it [00:34, 1288.72it/s]\u001b[A\n",
      "44809it [00:34, 1293.94it/s]\u001b[A\n",
      "44939it [00:34, 1283.69it/s]\u001b[A\n",
      "45068it [00:34, 1270.53it/s]\u001b[A\n",
      "45222it [00:34, 1340.36it/s]\u001b[A\n",
      "45358it [00:34, 1315.23it/s]\u001b[A\n",
      "45493it [00:35, 1324.10it/s]\u001b[A\n",
      "45627it [00:35, 1327.24it/s]\u001b[A\n",
      "45766it [00:35, 1344.68it/s]\u001b[A\n",
      "45901it [00:35, 1325.95it/s]\u001b[A\n",
      "46034it [00:35, 1239.69it/s]\u001b[A\n",
      "46170it [00:35, 1269.80it/s]\u001b[A\n",
      "46299it [00:35, 1249.24it/s]\u001b[A\n",
      "46425it [00:35, 1196.00it/s]\u001b[A\n",
      "46562it [00:35, 1243.01it/s]\u001b[A\n",
      "46688it [00:35, 1210.52it/s]\u001b[A\n",
      "46821it [00:36, 1243.54it/s]\u001b[A\n",
      "46968it [00:36, 1302.87it/s]\u001b[A\n",
      "47100it [00:36, 1306.41it/s]\u001b[A\n",
      "47232it [00:36, 1276.16it/s]\u001b[A\n",
      "47361it [00:36, 1201.61it/s]\u001b[A\n",
      "47502it [00:36, 1257.07it/s]\u001b[A\n",
      "47630it [00:36, 1219.39it/s]\u001b[A\n",
      "47754it [00:36, 1217.95it/s]\u001b[A\n",
      "47877it [00:36, 1158.84it/s]\u001b[A\n",
      "47997it [00:37, 1167.33it/s]\u001b[A\n",
      "48121it [00:37, 1187.39it/s]\u001b[A\n",
      "48260it [00:37, 1241.43it/s]\u001b[A\n",
      "48416it [00:37, 1321.13it/s]\u001b[A\n",
      "48559it [00:37, 1351.34it/s]\u001b[A\n",
      "48696it [00:37, 1322.52it/s]\u001b[A\n",
      "48838it [00:37, 1349.61it/s]\u001b[A\n",
      "48975it [00:37, 1300.67it/s]\u001b[A\n",
      "49110it [00:37, 1314.37it/s]\u001b[A\n",
      "49246it [00:37, 1327.22it/s]\u001b[A\n",
      "49383it [00:38, 1339.56it/s]\u001b[A\n",
      "49531it [00:38, 1377.39it/s]\u001b[A\n",
      "49673it [00:38, 1387.61it/s]\u001b[A\n",
      "49813it [00:38, 1321.75it/s]\u001b[A\n",
      "49947it [00:38, 1165.76it/s]\u001b[A\n",
      "50072it [00:38, 1189.14it/s]\u001b[A\n",
      "50194it [00:38, 1196.84it/s]\u001b[A\n",
      "50317it [00:38, 1203.91it/s]\u001b[A\n",
      "50453it [00:38, 1243.96it/s]\u001b[A\n",
      "50587it [00:39, 1270.30it/s]\u001b[A\n",
      "50716it [00:39, 1274.97it/s]\u001b[A\n",
      "50852it [00:39, 1296.94it/s]\u001b[A\n",
      "50985it [00:39, 1305.80it/s]\u001b[A\n",
      "51127it [00:39, 1333.24it/s]\u001b[A\n",
      "51261it [00:39, 1326.89it/s]\u001b[A\n",
      "51395it [00:39, 1330.52it/s]\u001b[A\n",
      "51529it [00:39, 1324.15it/s]\u001b[A\n",
      "51662it [00:39, 1306.73it/s]\u001b[A\n",
      "51810it [00:39, 1351.49it/s]\u001b[A\n",
      "51946it [00:40, 1343.27it/s]\u001b[A\n",
      "52087it [00:40, 1361.41it/s]\u001b[A\n",
      "145449it [02:03, 1179.88it/s]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from nltk.tokenize import word_tokenize\n",
    "from allennlp.data.tokenizers import Token\n",
    "from allennlp.data.fields import TextField, LabelField\n",
    "from typing import Iterator, List, Dict\n",
    "from allennlp.data import Instance\n",
    "from allennlp.data.dataset_readers import DatasetReader\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data import Vocabulary\n",
    "from allennlp.data.dataset import Batch\n",
    "\n",
    "\n",
    "class VerbDatasetReader(DatasetReader):\n",
    "\n",
    "    def __init__(self,sentence_indexers:Dict[str,TokenIndexer]=None )-> None:\n",
    "        super().__init__(lazy=False)\n",
    "        self.sentence_indexers=sentence_indexers or {\"sentence\":SingleIdTokenIndexer()}\n",
    "\n",
    "    def text_to_instance(self, claim: List[List], evidence: List[List], labels:str = None) -> Instance:\n",
    "        evidence_tokenized, claim_tokenized = [], []\n",
    "        for c in claim:\n",
    "            for word in word_tokenize(c):\n",
    "                claim_tokenized.append(Token(word))\n",
    "        for e in evidence:\n",
    "            for word in word_tokenize(e):\n",
    "                evidence_tokenized.append(Token(word))\n",
    "        claim_textfiled = TextField(claim_tokenized, self.sentence_indexers)\n",
    "        evidence_textfiled = TextField(evidence_tokenized, self.sentence_indexers)\n",
    "        fields={'claim':claim_textfiled, 'evidence':evidence_textfiled,'label': LabelField(labels)}\n",
    "        return Instance(fields)\n",
    "\n",
    "    def _read(self, file_path: str)->Iterator[Instance]:\n",
    "        mlinput_merge=pickle.load(open(file_path,'rb'))\n",
    "        for entry in mlinput_merge:\n",
    "            claim = [entry['claim']]\n",
    "            evidence = []\n",
    "            for sentence in entry['evidence']:\n",
    "                sentence = ' '.join(sentence)\n",
    "                evidence.append(sentence)\n",
    "            yield self.text_to_instance(claim, evidence, entry['label'])\n",
    "\n",
    "\n",
    "\n",
    "reader=VerbDatasetReader()\n",
    "\n",
    "instances = reader.read('mlinput_merge.txt')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': [The,\n",
       "  Fox,\n",
       "  Broadcasting,\n",
       "  Company,\n",
       "  -LRB-,\n",
       "  often,\n",
       "  shortened,\n",
       "  to,\n",
       "  Fox,\n",
       "  and,\n",
       "  stylized,\n",
       "  as,\n",
       "  FOX,\n",
       "  -RRB-,\n",
       "  is,\n",
       "  an,\n",
       "  American,\n",
       "  English,\n",
       "  language,\n",
       "  commercial,\n",
       "  broadcast,\n",
       "  television,\n",
       "  network,\n",
       "  that,\n",
       "  is,\n",
       "  owned,\n",
       "  by,\n",
       "  the,\n",
       "  Fox,\n",
       "  Entertainment,\n",
       "  Group,\n",
       "  subsidiary,\n",
       "  of,\n",
       "  21st,\n",
       "  Century,\n",
       "  Fox,\n",
       "  .,\n",
       "  He,\n",
       "  then,\n",
       "  played,\n",
       "  Detective,\n",
       "  John,\n",
       "  Amsterdam,\n",
       "  in,\n",
       "  the,\n",
       "  short-lived,\n",
       "  Fox,\n",
       "  television,\n",
       "  series,\n",
       "  New,\n",
       "  Amsterdam,\n",
       "  -LRB-,\n",
       "  2008,\n",
       "  -RRB-,\n",
       "  ,,\n",
       "  as,\n",
       "  well,\n",
       "  as,\n",
       "  appearing,\n",
       "  as,\n",
       "  Frank,\n",
       "  Pike,\n",
       "  in,\n",
       "  the,\n",
       "  2009,\n",
       "  Fox,\n",
       "  television,\n",
       "  film,\n",
       "  Virtuality,\n",
       "  ,,\n",
       "  originally,\n",
       "  intended,\n",
       "  as,\n",
       "  a,\n",
       "  pilot,\n",
       "  .],\n",
       " '_token_indexers': {'sentence': <allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer at 0x1348908d0>},\n",
       " '_indexed_tokens': None,\n",
       " '_indexer_name_to_indexed_token': None}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(instances[0].fields['evidence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 145449/145449 [00:06<00:00, 22157.31it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocabulary.from_instances(instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute '_six'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-100-0da6aae96704>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_field_embedders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBasicTextFieldEmbedder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mclaim_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vocab_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"claim\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mevidence_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vocab_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"evidence\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/allennlp/modules/token_embedders/embedding.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_embeddings, embedding_dim, projection_dim, weight, padding_index, trainable, max_norm, norm_type, scale_grad_by_freq, sparse, vocab_namespace, pretrained_file)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxavier_uniform_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m    551\u001b[0m                     \"cannot assign parameters before Module.__init__() call\")\n\u001b[1;32m    552\u001b[0m             \u001b[0mremove_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_parameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mregister_parameter\u001b[0;34m(self, name, param)\u001b[0m\n\u001b[1;32m    140\u001b[0m                 \"cannot assign parameter before Module.__init__() call\")\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m             raise TypeError(\"parameter name should be a string. \"\n\u001b[1;32m    144\u001b[0m                             \"Got {}\".format(torch.typename(name)))\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute '_six'"
     ]
    }
   ],
   "source": [
    "# Basic embeddings\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
    "\n",
    "claim_embedding = Embedding(num_embeddings=vocab.get_vocab_size(\"claim\"), embedding_dim=100, padding_index=0)\n",
    "evidence_embedding = Embedding(num_embeddings=vocab.get_vocab_size(\"evidence\"), embedding_dim=100, padding_index=0)\n",
    "\n",
    "# This is how the text_field_embedder knows which function to apply to which array. \n",
    "# There should be a 1-1 mapping between TokenIndexers and TokenEmbedders in your model.\n",
    "text_field_embedder = BasicTextFieldEmbedder({\"claim\": claim_embedding, \"evidence\": evidence_embedding})\n",
    "\n",
    "# the embedder maps the input tokens to the appropriate embedding matrix\n",
    "word_embeddings: TextFieldEmbedder = BasicTextFieldEmbedder({\"tokens\": token_embedding})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance with fields:\n",
      " \t evidece: TextField of length 85 with text: \n",
      " \t\t[Nikolaj, Coster-Waldau, worked, with, the, Fox, Broadcasting, Company, ., The, Fox, Broadcasting,\n",
      "\t\tCompany, -LRB-, often, shortened, to, Fox, and, stylized, as, FOX, -RRB-, is, an, American, English,\n",
      "\t\tlanguage, commercial, broadcast, television, network, that, is, owned, by, the, Fox, Entertainment,\n",
      "\t\tGroup, subsidiary, of, 21st, Century, Fox, ., He, then, played, Detective, John, Amsterdam, in, the,\n",
      "\t\tshort-lived, Fox, television, series, New, Amsterdam, -LRB-, 2008, -RRB-, ,, as, well, as,\n",
      "\t\tappearing, as, Frank, Pike, in, the, 2009, Fox, television, film, Virtuality, ,, originally,\n",
      "\t\tintended, as, a, pilot, .]\n",
      " \t\tand TokenIndexers : {'sentence': 'SingleIdTokenIndexer'} \n",
      " \t label: LabelField with label: SUPPORTS in namespace: 'labels'.' \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'claim': 'Nikolaj Coster-Waldau worked with the Fox Broadcasting Company.', 'label': 'SUPPORTS', 'evidence': [['The', 'Fox', 'Broadcasting', 'Company', '-LRB-', 'often', 'shortened', 'to', 'Fox', 'and', 'stylized', 'as', 'FOX', '-RRB-', 'is', 'an', 'American', 'English', 'language', 'commercial', 'broadcast', 'television', 'network', 'that', 'is', 'owned', 'by', 'the', 'Fox', 'Entertainment', 'Group', 'subsidiary', 'of', '21st', 'Century', 'Fox', '.\\n'], ['He', 'then', 'played', 'Detective', 'John', 'Amsterdam', 'in', 'the', 'short-lived', 'Fox', 'television', 'series', 'New', 'Amsterdam', '-LRB-', '2008', '-RRB-', ',', 'as', 'well', 'as', 'appearing', 'as', 'Frank', 'Pike', 'in', 'the', '2009', 'Fox', 'television', 'film', 'Virtuality', ',', 'originally', 'intended', 'as', 'a', 'pilot', '.\\n']]}\n"
     ]
    }
   ],
   "source": [
    "with open('mlinput_merge.txt', 'rb') as m:\n",
    "    a = pickle.load(m)\n",
    "    print(a[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestSemanticScholarDatasetReader(AllenNlpTestCase):\n",
    "    def test_read_from_file(self):\n",
    "        reader = SemanticScholarDatasetReader()\n",
    "        dataset = reader.read('tests/fixtures/s2_papers.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
